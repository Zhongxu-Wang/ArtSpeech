log_dir: "Outputs/LibriTTS"
first_stage_path: "epoch_1st_00028.pth"
save_freq: 4
log_interval: 100
device: "cuda"
multigpu: false
epochs_1st: 40 # number of epochs for first stage training
epochs_2nd: 200 # number of peochs for second stage training
batch_size: 32
pretrained_model: "" #"Outputs/LibriTTS/first_stage1_611.pth"
load_only_params: false # set to true if do not want to load epoch numbers and optimizer parameters

train_data: "Data/train_list_libritts_20.txt"
val_data: "Data/val_list_libritts.txt"
data_path: "C:/Users/汪中旭/Desktop/code_LibriTTS/Data/"

ASR_config: "Utils/ASR/config.yaml"
ASR_path: "Utils/ASR/epoch_00080.pth"

preprocess_params:
  sr: 24000
  spect_params:
    n_fft: 2048
    win_length: 1200
    hop_length: 300

model_params:
  hidden_dim: 512
  n_token: 178
  style_dim: 256
  n_layer: 3
  dim_in: 64
  max_conv_dim: 512
  n_mels: 80
  dropout: 0.2

loss_params:
    lambda_mel: 5. # mel reconstruction loss (1st & 2nd stage)
    lambda_adv: 1. # adversarial loss (1st & 2nd stage)
    lambda_reg: 1 # adversarial regularization loss (1st & 2nd stage)
    lambda_fm: 0.1 # feature matching loss (1st & 2nd stage)
    lambda_mono: 1. # monotonic alignment loss (1st stage, TMA)
    lambda_s2s: 1. # sequence-to-sequence loss (1st stage, TMA)
    lambda_params: 1.
    TMA_epoch: 20 # TMA starting epoch (1st stage)
    TMA_CEloss: false # whether to use cross-entropy (CE) loss for TMA
    lambda_F0: 1. # F0 reconstruction loss (2nd stage)
    lambda_norm: 1. # norm reconstruction loss (2nd stage)
    lambda_dur: 1 # duration loss (2nd stage)
    lambda_EMA: 1.
optimizer_params:
  lr: 0.0001
  pct_start: 0.0
