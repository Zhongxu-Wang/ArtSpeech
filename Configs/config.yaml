log_dir: "Outputs/LibriTTS"
first_stage_path: "first_stage.pth"
save_freq: 5
log_interval: 20
device: "cuda"
multigpu: false
epochs_1st: 120 # number of epochs for first stage training
epochs_2nd: 120 # number of peochs for second stage training
batch_size: 1
pretrained_model: "Outputs/LibriTTS/epoch_2nd_00119.pth"
load_only_params: false 
MAS_type: "legacy"

train_data: "Data/train_list_libritts_20.txt"
val_data: "Data/val_list_libritts.txt"

ASR_config: "Utils/ASR/config.yaml"
ASR_path: "Utils/ASR/epoch_00080.pth"

data_path: "../Data/"
stats_path: "Data/stats.json"

preprocess_params:
  sr: 24000
  spect_params:
    n_fft: 2048
    win_length: 1200
    hop_length: 300

model_params:
  hidden_dim: 512
  n_token: 178
  style_dim: 256
  n_layer: 3
  dim_in: 64
  max_conv_dim: 512
  n_mels: 80
  dropout: 0.2

loss_params:
    lambda_mel: 5. # mel reconstruction loss (1st & 2nd stage)
    lambda_adv: 1. # adversarial loss (1st & 2nd stage)
    lambda_reg: 1. # adversarial regularization loss (1st & 2nd stage)
    lambda_fm: 0.1 # feature matching loss (1st & 2nd stage)
    lambda_mono: 1. # monotonic alignment loss (1st stage, TMA)
    lambda_s2s: 1. # sequence-to-sequence loss (1st stage, TMA)
    lambda_params: 1.

    TMA_epoch: 20 # TMA starting epoch (1st stage)
    TMA_CEloss: false # whether to use cross-entropy (CE) loss for TMA

    lambda_F0: 1. # F0 reconstruction loss (2nd stage)
    lambda_norm: 1. # norm reconstruction loss (2nd stage)
    lambda_dur: 1. # duration loss (2nd stage)
    lambda_EMA: 1.

optimizer_params:
  lr: 0.00001
  pct_start: 0.0
